
        mwts-gcamera
        ---------------

INTRODUCTION 

The mwts-gcamera tests camera features of gstreamer.

Test are done to:       
  
Test scenarios include:

Questions: MWTSSupport@nokia.com

See document MWTS.README under doc directory for detailed general 
information about mwts test modules.


SOURCE CODE AND COMPILING

mwts-gcamera compiles on a typical red hat/linux distributions by executing 'sh rpmbuild.sh' if you have missing dependencies,
add them with 'zypper install <package>', if you need to search after a package you can do that with 'zypper search <package>'
and get more info with 'zypper info <package>' mwts-multimedia compiles on a typical debian/linux distributions by executing
'dpkg-buildpackage -rfakeroot', if you have missing dependencies add them with 'apt-get install <package>' on non-debian/red-hat
based linux system you can compile and install by (no dependency check) 'qmake', 'make', 'make install' (as root), if you have missing
sources you can track the package name with 'apt-file search <package>'.

Doxygen API documentation can be generated by running 'doxygen' in mwts-gcamera root directory.


ENVIRONMENT REQUIREMENTS

Set base paths for local and streaming into /usr/lib/tests/GCameraTest.conf.


EXECUTING TESTS

Tests are executed as "user", not "root". Check the MWTS.README for general instructions. Start 'min' in the shell of the device.


TEST RESULTS

Test results can be found from device in directory /var/log/tests/ Result files are named according to test cases, .result file contains overall
information about test run and measured values.

TEST CASES

MIN SCRIPT FILES

mwts-gcamera.cfg


MIN SCRIPTER INTERFACE

  SetFlags <SourceColorspaceConversion | SourceResize>
    It has to be the first call before SetPipeline, if it is needed. In case of capturing on netbook
    the v4l2src does not support those formats what the camerabin does. Hence colorspace conversion is needed.
    This might be expensive from the CPU point of view. (see more at Known Issues). It is also needed, if now
    optical zoom is avaiable so digital zoom has to be in use -> SourceResize

  SetPipeline
    It has to be the first call before after the test asset initialization. Everything has to be set beforhand

  TakeVideo <seconds>
    Start recording a video with given duration in seconds. Everything has to be set beforhand.

  TakePicture
    Takes a picture, it can be called once or more times if needed. SetPipeline has to be called before this.

  SetFPSMeasure 
    Measure framerate when doing video result
  	
  SetZoom <size>
   Set the zoom 100 < size < 1000. If not optical camera available, it can
   digital zoom (SetFlag SourceResize)

  SetImageResolution <x> <y> [fps] [fpsdivide]
	Set resolution and frame rate as optional. If video is being taken, 
    then the frame rate be given. Real frames per second = FPS / FPSDIVIDE      
  
  	Known image sizes and video rates:
    Video: Name, x, y, fps, fpsdivide
            QVGA,
            VGA, 640, 480, 25, 1
            CIF, 352, 288, 2997, 100
            QCIF, 176, 144, 1499, 100
            NTCS, 720, 486, 2999, 100
            PAL, 720, 576, 25, 1
            WCGA, 800, 480, 25, 1
  SetImageResolution <x> <y>
    Set resolution of the image.
    Image: Name, x, y
            VGA, 640, 480
            1,3Mpix (1280x960), 1280, 960
            3Mpix (2048x1536), 2048, 1536
            3,5Mpix 16:9 (2592x1456), 2592, 1458
            5Mpix (2592x1968), 2592, 1944
            5Mpix wide (2592x1458), 2592, 1458
        
    SetImagePostProcessing
     Use picture post prosessing, default postprocessing element is the "identity", it
     does nothing, but proves the funcionality
  
    SetVideoCodecs <video> <mux> <audiosrc> <audiocodec> <extension>
     Add codec handlers to pipeline. Order of arguments must be VIDEO, MUXER, AUDIOSOURCE
     , AUDIOCODEC, FILE_EXTENSION. VIDEO: theorenc MUXER: oggmux AUDIOSOURCE: pulsesrc
     (cpu expensive, depending on the test purpose), audiotestsrc (static generated sound
     is recorded, cheap) AUDIOCODEC: vorbisenc
 	    
	
TRACING

Log file can be found in /var/log/tests if the environment variable MWTS_DEBUG has been set to 1.
    Command: export MWTS_DEBUG=1


KNOWN ISSUES

The still image capture does not work without hack, because currently (24 Nov 2010) the MeeGo gst-plugins-bad-free package does
not have metadata muxer plugin (/usr/lib/gstreamer0.10/libmetadata.so), what is in default usage by the camerabin
http://bugs.meego.com/show_bug.cgi?id=10304.

This can be solved downloading the source gst-plugins-bad-free which has ext/metadata directory and compile the sources there
then copy the libmetadata.so to the /usr/lib/gstreamer-0.10.

The upstream gstreamer already does not use metadatamuxer but jifmux,
hence another solution to get the upstream gst-plugins-bad-free code and apply the following patch
http://cgit.freedesktop.org/gstreamer/gst-plugins-bad/commit/?id=9ae921f8889c662ddc95f8daba9a36a4801ebf66.

The following packages should be installed for building: libiptcdata-devel, exempi-devel


There are several available gstreamer tools from command line. For instance:
gst-inspect-0.10
gst-launch-0.10

gstreamer-tools package should be installed to get these. The gst-inspect-0.10 gives information about a module given as parameter:
gst-inspect-0.10 camerabin

It is possible to check whether the camerabin functional or not:
gst-launch-0.10 -v -m camerabin video-source=v4l2src viewfinder-sink=xvimagesink

If the gst-plugins-bad-free on the device is not upstream, then the default filter-caps are different. Hence the following should be used:
gst-launch-0.10 -v -m camerabin video-source=v4l2src filter-caps=video/x-raw-yuv viewfinder-sink=xvimagesink


OTHER RESOURCES  
                               
